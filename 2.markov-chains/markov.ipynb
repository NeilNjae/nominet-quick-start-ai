{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language generation and language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you give the next word in the following phrases?\n",
    "\n",
    "> Never gonna give you _\n",
    "> \n",
    "> That's one small step for a man, one _\n",
    "> \n",
    "> A bird in the hand is worth _\n",
    "> \n",
    "> London bridge is falling _\n",
    "> \n",
    "> To be or not to be, that is _\n",
    "> \n",
    "> It was the best of times, it was _\n",
    "> \n",
    "> The quick brown fox jumped _\n",
    "\n",
    "The chances are, you were able to give the next word, if not complete the whole phrase. This is because, in a lot of cases, language use is stereotyped. Certain words follow from certain phrases, and we can use that to help a machine work fluently with language.\n",
    "\n",
    "We can use this facility in a few ways. One is to help us understand human speech input. Often, the sounds we make while speaking can have more that one interpretation (try saying quickly \"recognise speech\" and \"wreck a nice beach\" to someone, and ask them which is which). In these cases, having some idea of the likely words can help us disambiguate the sounds the machine hears. We can also use the predictability of language to detect spelling and grammar mistakes; a grammar checker can detect incongurous words and suggest them for revision.\n",
    "\n",
    "Another application, one we'll be looking at here, is about _generating_ text that reads like a plausible new example of some source. If we build a language model using only text from one source (or a limited range of sources), that model will reflect that corpus of text. If we generate text with that model, it should have a similar style to the source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These applications rely on having a _language model_, a description of what the language should look like. There are many types of language model. You've probably heard of the \"large language models\" used by tools such as ChatGPT. For this example, we'll use a much smaller model, but it's surprising how good even this small, simple model can work.\n",
    "\n",
    "Our language model is inspired by the quiz above: if we know the last few words that have been used, we can make a prediction about what comes next. This is called an **_n_-gram** model in the literature, where _n_ is now many words of context we're using.\n",
    "\n",
    "For instance, let's we take the first line of _A Tale of Two Cities_\n",
    "\n",
    "> It was the best of times, it was the worst of times …\n",
    "\n",
    "We can build a 2-gram (bigram) model of this text. We slide a two-word-long window along the text and record, for each bigram, the word that comes next. Sliding the window looks a bit like this:\n",
    "\n",
    "| it | was | the | best | of | times | it | was | the | worst | of | times |\n",
    "|----|-----|-----|------|----|-------|----|-----|-----|-------|----|-------|\n",
    "| it | was | the |  |  |  |  |  |  |  |  |  |\n",
    "|  | was | the | best |  |  |  |  |  |  |  |  |\n",
    "|  |  | the | best | of |  |  |  |  |  |  |  |\n",
    "|  |  |  | best | of | times |  |  |  |  |  |  |\n",
    "|  |  |  |  | of | times | it |  |  |  |  |  |\n",
    "|  |  |  |  |  | times | it | was |  |  |  |  |\n",
    "|  |  |  |  |  |  | it | was | the |  |  |  |\n",
    "|  |  |  |  |  |  |  | was | the | worst |  |  |\n",
    "|  |  |  |  |  |  |  |  | the | worst | of |  |\n",
    "|  |  |  |  |  |  |  |  |  | worst | of | times |\n",
    "\n",
    "We can see that the bigram \"it was\" occurs twice in that sentence, and both times it is followed by the word \"the\". We can also see that the bigram \"was the\" occurs twice, but it is followed by different words each time: once by \"best\", once by \"worst\".\n",
    "\n",
    "The full bigram model from this sentence looks like this:\n",
    "\n",
    "* it was → the: 2\n",
    "* was the → best: 1, worst: 1\n",
    "* the best → of: 1\n",
    "* best of → times: 1\n",
    "* of times → it: 1, None: 1\n",
    "* times it → was: 1\n",
    "* the worst → of: 1\n",
    "* worst of → times: 1\n",
    "\n",
    "With only this short amount of text, the language model doesn't really tell us much interesting. We need more text. If we take the entire first chapter of the book, we find 878 bigrams, most of which occur only a couple of times. The most frequent ones are:\n",
    "\n",
    "* and a → knife: 1, queen: 2, thousand: 1\n",
    "* and seventy → five: 3\n",
    "* and the → fair: 1, farmer: 1, guard: 1, majesty: 1, mob: 1, musketeers: 1\n",
    "* by the → dozen: 1, other: 1, woodman: 1\n",
    "* hundred and → seventy: 3\n",
    "* in the → capital: 1, dark: 1, earthly: 1, hand: 1, life: 1, light: 1, midst: 1, rain: 1, rough: 1, superlative: 1, woods: 1\n",
    "* it was → clearer: 1, the: 11\n",
    "* of the → captain: 1, chickens: 1, cock: 1, common: 1, failure: 1, heavy: 1, large: 1, law: 1, plain: 1, revolution: 1, shield: 1, state: 1\n",
    "* on the → mob: 1, musketeers: 1, throne: 2, whole: 1\n",
    "* one thousand → seven: 3\n",
    "* seven hundred → and: 3\n",
    "* seventy five → conduct: 1, environed: 1, spiritual: 1\n",
    "* there were → a: 2, growing: 1, sheltered: 1\n",
    "* thousand seven → hundred: 3\n",
    "* to the → english: 1, human: 1, lords: 1\n",
    "* was the → age: 2, best: 1, epoch: 2, season: 2, spring: 1, winter: 1, worst: 1, year: 1\n",
    "* with a → fair: 1, high: 1, large: 2, plain: 1, sack: 1\n",
    "\n",
    "You can begin to see the flavour of Dickens in this model. For instance, the bigram \"was the\" shows that, at least in this chapter, Dickens was concerned with time and seasons. \n",
    "\n",
    "You should be able to see how we can use this model to generate text. If we're generating text, and we've just generated a particular _n_-gram, we can look up that _n_-gram in the language model and see the words that could come after it. We pick one of the listed words, weighted by how the probability of occurrence, and emit that word. That gives us a new _n_-gram, and the process repeats.\n",
    "\n",
    "For instance, let's say we start with the bigram \"it was\". We can look up words that come next, and the most most likey is \"the\". We emit that word and update the \"most recent bigram\" to be \"was the\". We pick one words that could follow: \"season\". Next comes \"of\", then a choice between \"Light\" and \"Darkness\". We can build up more text as we want by repeating the process.\n",
    "\n",
    "\n",
    "| Emitted text | Current bigram | Word choices | Chosen next word |\n",
    "|--------------|----------------|--------------|------------------|\n",
    "| it was | it was | clearer: 1, the: 9 | the |\n",
    "| it was the | was the | age: 2, best: 1, epoch: 2, season: 2, spring: 1, winter: 1, worst: 1, year: 1 | season |\n",
    "| it was the season | the season | of: 2 | of |\n",
    "| it was the season of | season of | Darkness: 1, Light: 1 | Darkness |\n",
    "| it was the season of Darkness | of Darkness | it: 1 | it |\n",
    "| it was the season of Darkness it | Darkness it | was: 1 | was |\n",
    "| it was the season of Darkness it was | it was | clearer: 1, the: 9 | the |\n",
    "| it was the season of Darkness it was the | was the | age: 2, best: 1, epoch: 2, season: 2, spring: 1, winter: 1, worst: 1, year: 1 | age |\n",
    "| it was the season of Darkness it was the age | the age | of: 2 | of |\n",
    "| it was the season of Darkness it was the age of | age of | foolishness: 1, wisdom: 1 | wisdom |\n",
    "| it was the season of Darkness it was the age of wisdom | of wisdom | it: 1 | it |\n",
    "| it was the season of Darkness it was the age of wisdom it | wisdom it | was: 1 | was |\n",
    "\n",
    "Now you have the idea of how the _n_-gram language model is built and used, it's time to implement it. This has three stages.\n",
    "\n",
    "1. Represent the language model\n",
    "2. Read some text and populate the model\n",
    "3. Use the model to generate new text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aside: reading text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to read large amount of text to populate our language models: a novel's-worth is about the minimum we can get away with. We need to split that text into words (and punctuation), and also split the text into sentences. (We'll generate one sentence at a time.)\n",
    "\n",
    "The reading and pre-processing this text is full of fiddly details that aren't worth going into. Instead, we'll just use these couple of functions to do the pre-processing for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import collections\n",
    "import unicodedata\n",
    "import random\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "token_pattern = re.compile(r'[^{}]+'.format(re.escape(string.ascii_letters + string.digits + string.punctuation)))\n",
    "punctuation_pattern = re.compile('(\\d+\\.\\d+|\\w+\\'\\w+|[{0}]+(?=\\w)|(?<=\\w)[{0}]+|[{0}]+$)'.format(re.escape(string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(text):\n",
    "    \"\"\"Split a text string into tokens, splitting on spaces and punctuation,\n",
    "    but keeping multiple punctuation characters as one token.\"\"\"\n",
    "    return [ch for gp in [re.split(punctuation_pattern, t) for t in re.split(token_pattern, text)]\n",
    "        for ch in gp if ch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def sjoin(tokens):\n",
    "    \"\"\"Combine a set of tokens into a string for pretty-printing.\"\"\"\n",
    "    sentence = ''\n",
    "    for t in tokens:\n",
    "        if t[-1] not in \".,:;')-!?\":\n",
    "            sentence += ' '\n",
    "        sentence += t\n",
    "    return sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'cat',\n",
       " 'sat',\n",
       " 'on',\n",
       " 'the',\n",
       " 'mat',\n",
       " '.',\n",
       " 'The',\n",
       " 'quick',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'jumped',\n",
       " 'over',\n",
       " 'the',\n",
       " 'lazy',\n",
       " 'dog',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = 'The cat sat on the mat. The quick brown fox jumped over the lazy dog.'\n",
    "tokenise(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The cat sat on the mat. The quick brown fox jumped over the lazy dog.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sjoin(tokenise(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we understand what the language model should look like, we can work out how to represent it in Python.\n",
    "\n",
    "The language model is a two-layered data structure. We have a bunch of _n_-grams; for each _n_-gram, we have a bunch of word choices; for each word choice we have a frequency of occurrence. These are key-value stores, so Python `dict`s are the obvious choice. That gives us a structure that looks like this:\n",
    "\n",
    "**Language model**\n",
    "| Key | Value |\n",
    "|-----|-------|\n",
    "| _n_-gram | word choices |\n",
    "\n",
    "**Word choices**\n",
    "| Key | Value |\n",
    "|-----|-------|\n",
    "| word | frequency |\n",
    "\n",
    "However, Python provides a couple of variations on `dict`s, in the [`collections`](https://docs.python.org/3/library/collections.html) library, that will make our lives easier.\n",
    "\n",
    "The first is a `Counter`, a `dict` specialised for counting things. We'll use this for counting the frequency of words. If we pass a sequence of things to a `Counter`, we get the counts of how often each thing occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 2, 'cat': 1, 'sat': 1, 'on': 1, 'mat': 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = collections.Counter(tokenise(\"the cat sat on the mat\"))\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we ask about a thing, we're told how often it occurs. Unknown keys don't generate an error, but return a count of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts['the'], counts['aardvark']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to count more things, we use the `update` method and pass in the new things to be counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 3,\n",
       "         'cat': 1,\n",
       "         'sat': 1,\n",
       "         'on': 1,\n",
       "         'mat': 1,\n",
       "         'quick': 1,\n",
       "         'brown': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.update(['the', 'quick', 'brown'])\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other useful `dict` variant is a `defaultdict`. This behaves exactly like a normal `dict` except it gives a default value if we ask for a missing key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(str, {3: 'hat', 6: 'banana'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = collections.defaultdict(str)\n",
    "dd[3] = 'hat'\n",
    "dd[6] = 'banana'\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hat', '')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd[3], dd[99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a `defaultdict` means we don't have to check an element exists before we update it.\n",
    "\n",
    "The other wrinkle is that Python won't let us use (mutable) lists of words as the keys to a `dict`-like structure, so we have to convert each _n_-grams from a `list` to a `tuple`.\n",
    "\n",
    "But with all those implementation details out of the way, let's get on with some programming!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buidling the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the utilities above, we can read a text file and split it into tokens. Our next job is to use that stream of tokens to build the language model.\n",
    "\n",
    "We'll build this up in stages, working from finding _n_-grams in a list to building the whole language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a piece of code that will find and print the trigrams (three-word slices) of `tokenise(sample_text)`. The last couple could well be shorter than three words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'cat', 'sat']\n",
      "['cat', 'sat', 'on']\n",
      "['sat', 'on', 'the']\n",
      "['on', 'the', 'mat']\n",
      "['the', 'mat', '.']\n",
      "['mat', '.', 'The']\n",
      "['.', 'The', 'quick']\n",
      "['The', 'quick', 'brown']\n",
      "['quick', 'brown', 'fox']\n",
      "['brown', 'fox', 'jumped']\n",
      "['fox', 'jumped', 'over']\n",
      "['jumped', 'over', 'the']\n",
      "['over', 'the', 'lazy']\n",
      "['the', 'lazy', 'dog']\n",
      "['lazy', 'dog', '.']\n",
      "['dog', '.']\n",
      "['.']\n"
     ]
    }
   ],
   "source": [
    "sentence = tokenise(sample_text)\n",
    "for i in range(len(sentence)):\n",
    "    print(sentence[i:i+3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify that code so it doesn't generate the final too-short trigram. This will mean stopping the loop earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'cat', 'sat']\n",
      "['cat', 'sat', 'on']\n",
      "['sat', 'on', 'the']\n",
      "['on', 'the', 'mat']\n",
      "['the', 'mat', '.']\n",
      "['mat', '.', 'The']\n",
      "['.', 'The', 'quick']\n",
      "['The', 'quick', 'brown']\n",
      "['quick', 'brown', 'fox']\n",
      "['brown', 'fox', 'jumped']\n",
      "['fox', 'jumped', 'over']\n",
      "['jumped', 'over', 'the']\n",
      "['over', 'the', 'lazy']\n",
      "['the', 'lazy', 'dog']\n",
      "['lazy', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = tokenise(sample_text)\n",
    "for i in range(len(sentence)-2):\n",
    "    print(sentence[i:i+3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the code above to find the bigram we want and the next token. Convert the bigram from a list to a tuple and store it in a variable `ngram`. Store the next token in a variable `next_token`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'cat') sat\n",
      "('cat', 'sat') on\n",
      "('sat', 'on') the\n",
      "('on', 'the') mat\n",
      "('the', 'mat') .\n",
      "('mat', '.') The\n",
      "('.', 'The') quick\n",
      "('The', 'quick') brown\n",
      "('quick', 'brown') fox\n",
      "('brown', 'fox') jumped\n",
      "('fox', 'jumped') over\n",
      "('jumped', 'over') the\n",
      "('over', 'the') lazy\n",
      "('the', 'lazy') dog\n",
      "('lazy', 'dog') .\n"
     ]
    }
   ],
   "source": [
    "sentence = tokenise(sample_text)\n",
    "for i in range(len(sentence)-2):\n",
    "    ngram = tuple(sentence[i:i+2])\n",
    "    next_token = sentence[i+2]\n",
    "    print(ngram, next_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now store these bigrams and next tokens in a language model.\n",
    "\n",
    "You can create an empty language model with the line\n",
    "\n",
    "```python\n",
    "model = collections.defaultdict(collections.Counter)\n",
    "```\n",
    "\n",
    "You can push these results into our language model, with the line\n",
    "\n",
    "```python\n",
    "model[ngram].update([next_token])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {('The', 'cat'): Counter({'sat': 1}),\n",
       "             ('cat', 'sat'): Counter({'on': 1}),\n",
       "             ('sat', 'on'): Counter({'the': 1}),\n",
       "             ('on', 'the'): Counter({'mat': 1}),\n",
       "             ('the', 'mat'): Counter({'.': 1}),\n",
       "             ('mat', '.'): Counter({'The': 1}),\n",
       "             ('.', 'The'): Counter({'quick': 1}),\n",
       "             ('The', 'quick'): Counter({'brown': 1}),\n",
       "             ('quick', 'brown'): Counter({'fox': 1}),\n",
       "             ('brown', 'fox'): Counter({'jumped': 1}),\n",
       "             ('fox', 'jumped'): Counter({'over': 1}),\n",
       "             ('jumped', 'over'): Counter({'the': 1}),\n",
       "             ('over', 'the'): Counter({'lazy': 1}),\n",
       "             ('the', 'lazy'): Counter({'dog': 1}),\n",
       "             ('lazy', 'dog'): Counter({'.': 1})})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = tokenise(sample_text)\n",
    "model = collections.defaultdict(collections.Counter)\n",
    "for i in range(len(sentence)-2):\n",
    "    ngram = tuple(sentence[i:i+2])\n",
    "    next_token = sentence[i+2]\n",
    "    model[ngram].update([next_token])\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to take the code we've written and wrap it in a function definition, to make it easy to call for each sentence we process.\n",
    "\n",
    "While we're at it, we get rid of the \"magic number\" 2 in the code, and replace it with a parameter for the tuple size.\n",
    "\n",
    "That gives us the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(text, tuple_size=2):\n",
    "    model = collections.defaultdict(collections.Counter)\n",
    "    # Record each n-gram in turn\n",
    "    for i in range(len(text) - tuple_size):\n",
    "        n_gram = text[i:i+tuple_size]\n",
    "        next_word = text[i+tuple_size]\n",
    "        model[tuple(n_gram)].update([next_word])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {('The', 'cat'): Counter({'sat': 1}),\n",
       "             ('cat', 'sat'): Counter({'on': 1}),\n",
       "             ('sat', 'on'): Counter({'the': 1}),\n",
       "             ('on', 'the'): Counter({'mat': 1}),\n",
       "             ('the', 'mat'): Counter({'.': 1}),\n",
       "             ('mat', '.'): Counter({'The': 1}),\n",
       "             ('.', 'The'): Counter({'quick': 1}),\n",
       "             ('The', 'quick'): Counter({'brown': 1}),\n",
       "             ('quick', 'brown'): Counter({'fox': 1}),\n",
       "             ('brown', 'fox'): Counter({'jumped': 1}),\n",
       "             ('fox', 'jumped'): Counter({'over': 1}),\n",
       "             ('jumped', 'over'): Counter({'the': 1}),\n",
       "             ('over', 'the'): Counter({'lazy': 1}),\n",
       "             ('the', 'lazy'): Counter({'dog': 1}),\n",
       "             ('lazy', 'dog'): Counter({'.': 1})})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model = build_model(tokenise(sample_text))\n",
    "sample_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "967"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_chapter = open('tale-of-two-cities.txt').read()[1882:7653]\n",
    "sample_chapter_model = build_model(tokenise(sample_chapter.lower()), tuple_size=2)\n",
    "len(sample_chapter_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 11, 'clearer': 1})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_chapter_model[('it', 'was')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a model, we can use it to generate text.\n",
    "\n",
    "The process is roughly the reverse of how we created the language model. We have a current _n_-gram. We find that _n_-gram in the language model, look up the possible next tokens, and pick one. We update the current _n_-gram to include this next token, and the process repeats. Meanwhile, we keep track of all the generated tokens.\n",
    "\n",
    "A typical generation run is below. You can see how the language model guides the generation of the text.\n",
    "\n",
    "| Generated text | Current _n_-gram | Next token options | Chosen token |\n",
    "|----------------|------------------|--------------------|--------------|\n",
    "| it was | it was | the → 11, clearer → 1 | the |\n",
    "| it was the | was the | age → 2, epoch → 2, season → 2, best → 1, worst → 1, spring → 1, winter → 1, year → 1 | season |\n",
    "| it was the season | the season | of → 2 | of |\n",
    "| it was the season of | season of | light → 1, darkness → 1 | darkness |\n",
    "| it was the season of darkness | of darkness | , → 1 | , |\n",
    "| it was the season of darkness , | darkness , | it → 1 | it |\n",
    "| it was the season of darkness , it | , it | was → 9 | was |\n",
    "| it was the season of darkness , it was | it was | the → 11, clearer → 1 | the |\n",
    "| it was the season of darkness , it was the | was the | age → 2, epoch → 2, season → 2, best → 1, worst → 1, spring → 1, winter → 1, year → 1 | age |\n",
    "| it was the season of darkness , it was the age | the age | of → 2 | of |\n",
    "| it was the season of darkness , it was the age of | age of | wisdom → 1, foolishness → 1 | foolishness |\n",
    "| it was the season of darkness , it was the age of foolishness | of foolishness | , → 1 | , |\n",
    "| it was the season of darkness , it was the age of foolishness , | foolishness , | it → 1 | it |\n",
    "| it was the season of darkness , it was the age of foolishness , it | , it | was → 9 | was |\n",
    "| it was the season of darkness , it was the age of foolishness , it was | it was | the → 11, clearer → 1 | the |\n",
    "| it was the season of darkness , it was the age of foolishness , it was the | was the | age → 2, epoch → 2, season → 2, best → 1, worst → 1, spring → 1, winter → 1, year → 1 | year |\n",
    "| it was the season of darkness , it was the age of foolishness , it was the year | the year | of → 1, one → 1 | of |\n",
    "| it was the season of darkness , it was the age of foolishness , it was the year of | year of | our → 1 | our |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking a random next token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we need to do is pick a suitable next token, given a particular _n_-gram and a langauge model.\n",
    "\n",
    "Python's built-in `random` library has a function `choice()` that will select a random element from a list.\n",
    "\n",
    "The `Counter` object has a method `elements()` that will return all the items in the `Counter`, each appearing as many times as its count. `elements()` returns an _iterator_, so we need to wrap it in a call to `list` to convert it to the list that `choice` needs.\n",
    "\n",
    "This cell generates all possible next words for a given _n_-gram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best',\n",
       " 'worst',\n",
       " 'age',\n",
       " 'age',\n",
       " 'epoch',\n",
       " 'epoch',\n",
       " 'season',\n",
       " 'season',\n",
       " 'spring',\n",
       " 'winter',\n",
       " 'year']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sample_chapter_model[('was', 'the')].elements())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell picks one of them at random. If you run this cell a few times, you should see different results most of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'season'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(list(sample_chapter_model[('was', 'the')].elements()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(There are other choices for how to select the next item, but we won't go into that here.)\n",
    "\n",
    "This is the procedure that will generate text for us. The body of it is the `while` loop, that generates a new token while the current _n_-gram exists in the language model, and the generated text isn't longer than the limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, starting_ngram=None, max_length=500):\n",
    "    if starting_ngram:\n",
    "        current = starting_ngram\n",
    "    else:\n",
    "        current = random.choice(list(model))\n",
    "    generated = list(current)\n",
    "    while current in model and len(generated) < max_length:\n",
    "        next_item = random.choice(list(model[current].elements()))\n",
    "        # print(generated, ':', current, ':', model[current], ':', next_item)\n",
    "        generated.append(next_item)\n",
    "        current = current[1:] + (next_item, )\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this with the `sample_model` created above. This will test the procedure runs without errors, but doesn't produce exciting text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The cat sat on the mat. The quick brown fox jumped over the lazy dog.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sjoin(generate_text(sample_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the first chapter of _A Tale of Two Cities\". For information, we show how many distinct bigrams are in this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "967"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_chapter = open('tale-of-two-cities.txt').read()[1882:7653]\n",
    "sample_chapter_model = build_model(tokenise(sample_chapter.lower()), tuple_size=2)\n",
    "len(sample_chapter_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate som text, starting with the same opening phrase. We limit the output to 20 tokens, but you can increase it if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it was the season of light, and the majesty of the common way. in england, there were'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sjoin(generate_text(sample_chapter_model, starting_ngram=('it', 'was'), max_length=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this seeming to work, let's load the whole book…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128671"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_cities = open('tale-of-two-cities.txt').read()\n",
    "two_cities_model = build_model(tokenise(two_cities), tuple_size=3)\n",
    "len(two_cities_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "…and generate some text. Run this cell several times, and you'll see different text generated each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'towards the latter precaution, by giving out that the Doctor himself sat reading at the window. \" How goes it, Jacques ?\" \" All sorts of people have been similarly buried in worse prisons, before now .\" \" He told me that he disliked me deeply, for knowing what I knew from the boy. He was buried in London, sir ?\" \" Am I not ?\" \" I have been surrounded all day long, have made me nervous without reason. You are not\"-- the apprehension came suddenly into his mind'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sjoin(generate_text(two_cities_model,  max_length=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just printing the text gives annoying breaks in the middle of words. If we produce HTML text, the browser will make it prettier for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint(tokens):\n",
    "    display(HTML(sjoin(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "at least nothing more than to get to sleep again, which the man dexterously combated by stirring the fire continuously for five minutes, he got it down. \" There's all manner of gestures while he spoke, and he is lame. Because he is lame. Because he had voluntarily relinquished a title that was distasteful to him, or try to get their straying thoughts together. Only one soul was to be found .\" The hands of the man who had spoken, and Mr Attorney- General had to inform the society--"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(generate_text(two_cities_model,  max_length=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95956"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odyssey = open('odyssey.txt').read()\n",
    "odyssey_model = build_model(tokenise(odyssey), tuple_size=3)\n",
    "len(odyssey_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109621"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pride = open('pride-and-prejudice.txt').read()\n",
    "pride_model = build_model(tokenise(pride), tuple_size=3)\n",
    "len(pride_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104656"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arthur = open('le-mort-d-arthur.txt').read()\n",
    "arthur_model = build_model(tokenise(arthur), tuple_size=3)\n",
    "len(arthur_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Penelope has said that I am safe and have returned from Pylos\"; but Eumaeus went close up to us; some god fooled me into setting off with nothing on but my shirt, and he with them. It was not long ere Penelope came to know what the suitors are eating up my house under the pretext of paying their addresses to my mother. You will thus see how much my ships excel all others in weaving, for Minerva had beautified him about the head and shoulders just as a skilful workman who has studied art of all kinds, such as cables and sails, and putting by their oars; so we got away with those that were left. \" My friend ,\" said he,' you are cruel; you are a tall smart- looking fellow show your mettle, then, of me your poor servant who calls upon you, and who made the Phaeacians take so great a liking to you as though he were dead, if he goes near Ulysses' house, and she proved to be of an excellent disposition. Still, he shall have plenty of hardship yet before he has done you. Besides, I cannot think whatever I am to escape to from their avengers when it is boiling over upon a great fire, and then went back to the house and take them away: let the cupbearer go round with his cups, that we might get some meat for our dinner. On this Amphinomus said, \" It rests with heaven. But do you, therefore, and take another yourself, and let him see them. If he had been used to roughing it, nor to their father either when he is in the house, and spoke kindly to each of them till he had got his breath and came to the house of Deiphobus. It was not that he too lifts his hands in front of him, for he was killed in Thebes by reason of a woman's gifts. His sons as they left their rooms gathered round him, and kissed him, saying,' how this man gets honoured and makes friends to whatever city or country he may go. See what fine prizes he is taking home from Troy, while we, who have lost my brave and lion- hearted husband, who had been killed by Agamemnon's son Orestes; so he made the sons marry the daughters, and they raised a dust upon the plain of Troy, he would have gone off with any of them and you must tell your men to do so. When it was about to reach the high promontory of Malea, he was Mentes, son of king Apheidas, who is coming home,"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(generate_text(odyssey_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       ", imbued from her childhood with a brooding look upon them that looked ill. In the instinctive association of prisoners with shameful crime and disgrace, the new postilions follow, sucking and plaiting the lashes of their whips; the valet walked by the horses; the courier was audible, trotting on ahead into the dull distance. At the appointed hour, he emerged from it to the noise conventionally assigned to the owl by men- poets. But it really is doubly and trebly hard to have crowds and multitudes of people turning up after him ( I could have gained nothing by it. There were four in the room, shook his head at him in a whisper, of the King's Bench bar ?\" \" Do you seek any promise from me ?\" \" I ask none, sir. Your honour told me to call you .\" \" Come, come !\" said he. In a similar way I am, sir !\" Mr Lorry followed Sydney to the outer door, and Jerry followed him, when not engaged in making forays through the Bar, Mr Cruncher made the observation to his next neighbour, and then all cast down their eyes and sat silent. Except one man, who got up and hurried away. By good fortune, too, it behoved him to present himself in Soho, that this gaoler was so unwholesomely bloated, both in the old way. Finally, he had no recollection whatever of his having been a mere boy in the Doctor's service, and of constantly making fresh additions to his stock of knowledge, which was turned to the followers of the Good Republican Brutus of Antiquity, not far off, on which there had been any eyes to notice the influence of the Loadstone Rock Book the Third-- the Track of a Storm CHAPTER I In Secret CHAPTER II The Grindstone CHAPTER III The Night Shadows CHAPTER IV The Preparation CHAPTER V The Wood- Sawyer One year and three months when the Doctor was accustomed to be with her, and when you knew it. You may also tell him, from me, weep for it, they came to a stop, besides once drawing the coach across the road behind him, and evidently disappointed by the dialogue taking a turn that did not seem to promise bloodshed, held by Defarge's arm as he held by the turnkey's. Their three heads had been close together during this brief discourse, and it was an hour later .\" They went into a dingy room lined with books and littered with papers, where there was a little man with a grey head, who had never struck a blow in her life. But, not for a week or two before Michaelmas Term,"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(generate_text(two_cities_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "tenderness, however, at last, on examining their watches, that it was no more than she felt for herself. Her astonishment at his coming to Netherfield, because the Miss Bennets should meet to talk over a ball was, at any rate there seemed a gulf impassable between them. Had Lydia's marriage been concluded on the most honourable terms, it was given to another man; and of late it has been the means of removing all my doubts. I am extremely glad that you have such pleasant accounts from our friends at Hunsford. Such doings discomposed Mr Bennet exceedingly. In his present behaviour to herself could now have had no tolerable motive: he had either been deceived with regard to new furniture. Elizabeth, equally next to Jane in birth and beauty, and that Longbourn estate is just as he chooses. Nobody wants him to come; and it might strike them that they could not succeed. By supposing such an affection, you make everybody acting unnaturally and wrong, and because I am a married woman .\" It was absolutely necessary to interrupt him now. \" You have a house of my own child; but to be so easily reconciled to myself. After this day, Jane said no more. Mrs Gardiner had formed, of their becoming hereafter her own. Till the next morning with Elizabeth in pursuit of the officers always made part of it, than hurrying into the little copse, where she was welcomed by her two friends, and were traced almost to London, before they left the high road for the lane to Hunsford, and I know I appeared distressed; but don't imagine it was from any silly cause. I was sometimes quite provoked; but then I recollected my dear Elizabeth ,\" she added; \" but at the same time to supply him with fishing tackle, and pointing out those parts of the story, however, to have the attempt known till its success could be known likewise; for, on first hearing it, Mrs Bennet ?\" \" Yes, Miss Elizabeth ,\" said her father, \" to guess in what way he can mean to make us the atonement he thinks our due, the wish is certainly to his credit .\" Elizabeth was distressed. She felt that she could be an object of admiration to so great a man, the ladies all felt that he would. This event had at last been despaired of, but it could not be enough detested, had given something more of fretfulness than usual to himself. He was not seated by her: perhaps that was the reason of things, that their brother is happy with me, in considering a removal from that corps"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(generate_text(pride_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "his way. And anon as this little brachet felt a savour of Sir Tristram. And therewithal came Sir Tristram de Liones rescued that shield from her, For thou hast done me, and to this court he came leaning upon two men's shoulders, as though he might not handle it. Then Arthur went to the brains, and the queen by Merlin's advice, said the Damosel of the Lake and brought with them King Arthur, and the Red Knight, Stand up, and took the lady away with him as fast as they might. So the knight of Ireland, encountered with Sir Uwaine. And he gave to Sir Priamus the duchy of Lorraine; and he asked King Arthur if he would give no mercy unto the knight, that lay sick in his bed a merry lay, such one heard they never none in Ireland before that time La Beale Isoud was in the forest a- hunting, upon a day Queen Isoud walked into the forest to wit what was that noise. And so when he was mad in the forest had reared up a pavilion by a well, and brought him with him to his castle. Now will we turn unto Sir Palomides by Gouvernail, and how he had passed all the perilous passages. What manner a knight is yonder knight that seemeth in so many colours, well hast thou jousted; now make thee ready that I may better withstand than he did, and whether that he were mightily withstood. It is evil seen, said Sir Kay, the Seneschal, Sir, I cannot tell; wherefore I beseech you make me knight when I require him to joust with one of them held a taper of wax that burnt day and night he desireth of me to be there, and knightly he rode forth through the fair countries and cities, and found a lady lying sleeping therein, but there might none cast bar nor stone to him by two yards. Then would I have been at the court of King Arthur. By two knights, Ye say truth, and so passed the sea and rode toward Gawaine with his fellows so quit them that they had ado with Sir Tristram, that is great pity of their wilfulness. Then all the barons. I shall take thee. Then Sir Marhaus departed and was led to a chamber, a richer beseen chamber saw he never none, and so he took his horse and stole away from his uncle's court. But she was a great jousts and a great spear of his squire, that was great dishonour to Sir Palomides, but he and his men were passing well beseen at all points. So God me help, said the forester"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(generate_text(arthur_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have several language models. As you've seen, each generates text in the style of the source text. What happens if we combine models?\n",
    "\n",
    "The mechanics of this are fairly easy. The `Counter`s we're using can be added with the `+` operator. This does what you'd expect, and adds all the counts of the two `Counter`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = collections.Counter(tokenise(\"the cat sat on the mat\"))\n",
    "c2 = collections.Counter(tokenise(\"the cat lay on the bed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({'the': 2, 'cat': 1, 'sat': 1, 'on': 1, 'mat': 1}),\n",
       " Counter({'the': 2, 'cat': 1, 'lay': 1, 'on': 1, 'bed': 1}),\n",
       " Counter({'the': 4,\n",
       "          'cat': 2,\n",
       "          'on': 2,\n",
       "          'sat': 1,\n",
       "          'mat': 1,\n",
       "          'lay': 1,\n",
       "          'bed': 1}))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1, c2, c1 + c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the two models below, create a **new** model that combines them. The result should be:\n",
    "\n",
    "```python\n",
    "defaultdict(collections.Counter,\n",
    "            {('the', 'cat'): Counter({'sat': 1, 'lay': 1}),\n",
    "             ('cat', 'sat'): Counter({'on': 1}),\n",
    "             ('sat', 'on'): Counter({'the': 1}),\n",
    "             ('on', 'the'): Counter({'mat': 1, 'bed': 1}),\n",
    "             ('cat', 'lay'): Counter({'on': 1}),\n",
    "             ('lay', 'on'): Counter({'the': 1})})\n",
    "```\n",
    "\n",
    "(The order of elements in the model may vary, but the contents should be the same.)\n",
    "\n",
    "Ensure that both source models remain unchanged by the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(defaultdict(collections.Counter,\n",
       "             {('the', 'cat'): Counter({'sat': 1}),\n",
       "              ('cat', 'sat'): Counter({'on': 1}),\n",
       "              ('sat', 'on'): Counter({'the': 1}),\n",
       "              ('on', 'the'): Counter({'mat': 1})}),\n",
       " defaultdict(collections.Counter,\n",
       "             {('the', 'cat'): Counter({'lay': 1}),\n",
       "              ('cat', 'lay'): Counter({'on': 1}),\n",
       "              ('lay', 'on'): Counter({'the': 1}),\n",
       "              ('on', 'the'): Counter({'bed': 1})}))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = build_model(tokenise(\"the cat sat on the mat\"))\n",
    "m2 = build_model(tokenise(\"the cat lay on the bed\"))\n",
    "m1, m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(defaultdict(collections.Counter,\n",
       "             {('the', 'cat'): Counter({'sat': 1, 'lay': 1}),\n",
       "              ('cat', 'sat'): Counter({'on': 1}),\n",
       "              ('sat', 'on'): Counter({'the': 1}),\n",
       "              ('on', 'the'): Counter({'mat': 1, 'bed': 1}),\n",
       "              ('cat', 'lay'): Counter({'on': 1}),\n",
       "              ('lay', 'on'): Counter({'the': 1})}),\n",
       " defaultdict(collections.Counter,\n",
       "             {('the', 'cat'): Counter({'sat': 1}),\n",
       "              ('cat', 'sat'): Counter({'on': 1}),\n",
       "              ('sat', 'on'): Counter({'the': 1}),\n",
       "              ('on', 'the'): Counter({'mat': 1})}),\n",
       " defaultdict(collections.Counter,\n",
       "             {('the', 'cat'): Counter({'lay': 1}),\n",
       "              ('cat', 'lay'): Counter({'on': 1}),\n",
       "              ('lay', 'on'): Counter({'the': 1}),\n",
       "              ('on', 'the'): Counter({'bed': 1})}))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m12 = collections.defaultdict(collections.Counter)\n",
    "for k in m1:\n",
    "    m12[k] += m1[k]\n",
    "for k in m2:\n",
    "    m12[k] += m2[k]\n",
    "m12, m1, m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_models(model1, model2):\n",
    "    merged = collections.defaultdict(collections.Counter)\n",
    "    for k in model1:\n",
    "        merged[k] += model1[k]\n",
    "    for k in model2:\n",
    "        merged[k] += model2[k]\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "last night acknowledged to have required the utmost force of passion to put aside in my own opinion, constitute my happiness, without reference to _ you_, whose very countenance may vouch for your being unable to do either ?\" He answered, in a voice of dire exasperation: \" Bust me, if I had not an idea of it .\" Neither of them for sending it to England .\" \" Without it, can I not save you, Mr Carton, who had looked into the red coals at the Royal George, that although but one kind of man was seen to go into the avenue .\" But Elizabeth, who had probably been his own companion from childhood, connected together, as I saw them all that Colonel Fitzwilliam came because he had not a temper to bear inconveniences-- cheerfulness to enhance every pleasure-- and affection and intelligence, which might once have been handsome. Her air was not conciliating, nor was there any revival of past occurrences, or any alarm; and within five miles of Lambton, the scene of Mrs Gardiner's former residence, and where she felt herself to be so; I knew you would be amused, Mr Darnay; may be not. Don't let your sober face elate you, however; you don't know what reason there is in it. He and his books-- these, and the stress that is laid on them, as a blind, at a certain convenient height from the ground. \" The rain- drops on it. The strong tide, so swift, so deep, and certain, was like recalling some very weak person from a swoon, or endeavouring, in the general conversation: but Elizabeth felt an anxiety on the subject. In the morning, he was taken with that peculiar kind of short cough requiring the hollow of the right, and then he had nothing of a pleasant nature to send. It was curious. The moment I come to you, sir, to half a song, and then confiscated. For, he contended with himself that it might have been prevailed on to overlook the offence, and seek a reconciliation; and, in the modesty and virtue of our sisters, many years, but we don't know what it is .\" \" Yes, sir, at another word, might stay a month .\" \" You are over scrupulous, surely. I dare say you will be delighted with her. He was the proudest, most disagreeable man in the world if he did not admire her at all, and explained his worldly affairs. That done, with many expressions of concern, he confessed himself engaged elsewhere. \" Next time you call"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "two_cities_pride_model = merge_models(two_cities_model, pride_model)\n",
    "pprint(generate_text(two_cities_pride_model)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "on their left hand an eagle with a great white goose in its talons, and the eight great towers, cannon, muskets, fire and smoke. One drawbridge down! \" Work, comrades all, work! Work, Jacques One Thousand, Jacques Two Thousand, Jacques Two Thousand, Jacques Five !\" The mender of roads sat on the heap of stones. He was buried in London, at the time, and all the tides and winds were setting straight and strong towards it. He married her after having killed his father's murderer Aegisthus? You are a turnkey at the Conciergerie ?\" \" I wish the stranger had gone somewhere else, for I had never let the rudder out of my soul. For you, and in the meantime Ulysses and the swineherd came up to him and he received it gladly. Then Minerva said to Jove, \" Father, son of Iasus, who was lifted up to clasp her in my arms; it stood between the little grated window and the door. \" Good night, Mr Darnay; may be not. Don't let your sober face elate you, however; and the apparitions vanished from his sight forever. The wicket opened on a stone staircase, leading upward. When they were within he took her hand in his own house, feasting with his wife to induce her to admit it, \" that he keeps that reminder of his sufferings about him !\" \" Arrested for all that, and the prisoner a hundred times worse. Lastly, came my Lord himself arose from his seat, and was none the better for it in pocket; and it's my suspicion that you've been praying again me, and let me have no more to do in this house with a stranger, so did his heart growl with anger at the evil deeds that were to be tried together, next, as enemies of the Republic crouching in its bosom, the English traitor and agent of all mischief so much spoken of and so difficult to trace, that I should be too cold, and had held the impoverished and involved estate on written instructions, to spare the people, but live in caves on the tops of high mountains; each is lord and master in his family, stared him so reproachfully in the face, you, in your native country, an official among foreigners, and such delicate honour to the sense of smell, would surely keep anything going, for Circe has told me all that you have just told him, and already broke upon the neighbouring beach, or the manner. There was a great man because you live such a long time past. Ulysses has at last come"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "two_cities_odyssey_model = merge_models(two_cities_model, odyssey_model)\n",
    "pprint(generate_text(two_cities_odyssey_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "with me here at my place, it is shame to all those things. Where shall I find him? In such a place, said Sir Tristram, that as men say he hath seven men's strength. God save you, said Griflet, I beseech you make me knight. Ah, lady, said Dame Lionesse, and how for her sake. Thus was he kept until day. And tell King Mark that was ridden from his men all alone. When they heard he was escaped then they were honoured with a call at the Parsonage; for Mr Collins was punctual to his appointment; and he said, Sir, this shall never prove none such. For Sir Epinogrus was the first grand object of every morning's impatience. Through letters, whatever of that kind. But, though this might be imaginary, she could not but triumph. It was as follows:-- We have reason to think Bingley very much indebted to him. Then the Lady of the Rock, the which pleased neither of them; and to _ her_, had she been his relation. Lady Catherine approached, and saw a damosel going upon the lake. What damosel is that? said Sir Launcelot, and assailed him; and Beaumains came as fast upon him with his horse struck the Green Knight's horse upon the side, and the first mass was done, then came in Sir Priamus with his pennon, and rode toward the castle where they should do deeds of worship; and here I shall not fail you, by the grace of God, and depart the goods among you. And therewith he departed from them, to come to his bed. And because Sir Dinadan had a fall, and was impatient for more. Mrs Gardiner was surprised and concerned: but as they were wild, so that three days and three nights. Then at the last Sir Tristram doubled his strokes, and there Sir Palomides had all the appearance of his goodness. Elizabeth, immediately recognizing the livery, guessed what it meant, and imparted no small degree of surprise to her relations, with only one serious parting look, went away. Have you never happened to see her! and to see dear Wickham too! But the clothes, the wedding need not be put off, for Mr Darcy might leave the country. Then for very pure love he might never be whole till he came unto the Castle Perilous; and here my younger brother said he would never have come so soon to wait upon me .\" Elizabeth had scarcely time to disclaim all right to the compliment before their approach was announced by the door- bell, and Miss Elizabeth was summoned"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arthur_pride_model = merge_models(arthur_model, pride_model)\n",
    "pprint(generate_text(arthur_pride_model, max_length=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgements\n",
    "\n",
    "All the source texts used here come from [Project Gutenberg](https://www.gutenberg.org/), an online source of public domain works. https://www.gutenberg.org/policy/permission.htmlI've modified the books slightly from the versions available there, to remove the legal licence boilerplate and convert some characters to ASCII equivalents. "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
